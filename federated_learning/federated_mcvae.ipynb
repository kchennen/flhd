{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaUlM5G0-3Z2"
   },
   "source": [
    "# Federated Variational Autoencoders\n",
    "We are going to study an example of federated latent variable modeling using federated learning and Variational autoencoders. In this example we will illustrate an iid scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "R4I97nmo-yaj"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3TAtgoun-o-I"
   },
   "outputs": [],
   "source": [
    "N_CENTERS = 4\n",
    "N_ROUNDS = 10   # Number of iterations between all the centers training and the aggregation process.\n",
    "\n",
    "N_EPOCHS = 15   # Number of epochs before aggregating\n",
    "BATCH_SIZE = 48\n",
    "LR = 1e-3       # Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rwlv7bhp4Pu1"
   },
   "source": [
    "We define a set of functions to distribute our dataset across multiple centers (`split_iid`) and for computing the federated averaging (`federated_averaging`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0a2H2aMK_XEJ"
   },
   "outputs": [],
   "source": [
    "def split_iid(dataset, n_centers):\n",
    "    \"\"\" Split PyTorch dataset randomly into n_centers \"\"\"\n",
    "    n_obs_per_center = [len(dataset) // n_centers for _ in range(n_centers)]\n",
    "    return random_split(dataset, n_obs_per_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BwVx6Cda4uyQ"
   },
   "outputs": [],
   "source": [
    "def federated_averaging(models, n_obs_per_client):\n",
    "    assert len(models) > 0, 'An empty list of models was passed.'\n",
    "    assert len(n_obs_per_client) == len(models), 'List with number of observations must have ' \\\n",
    "                                                 'the same number of elements that list of models.'\n",
    "\n",
    "    # Compute proportions\n",
    "    n_obs = sum(n_obs_per_client)\n",
    "    proportions = [n_k / n_obs for n_k in n_obs_per_client]\n",
    "\n",
    "    # Empty model parameter dictionary\n",
    "    avg_params = models[0].state_dict()\n",
    "    for key, val in avg_params.items():\n",
    "        avg_params[key] = torch.zeros_like(val)\n",
    "\n",
    "    # Compute average\n",
    "    for model, proportion in zip(models, proportions):\n",
    "        for key in avg_params.keys():\n",
    "            avg_params[key] += proportion * model.state_dict()[key]\n",
    "\n",
    "    # Copy one of the models and load trained params\n",
    "    avg_model = copy.deepcopy(models[0])\n",
    "    avg_model.load_state_dict(avg_params)\n",
    "\n",
    "    return avg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtBeNOs_5TRV"
   },
   "source": [
    "## Federating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zQIM1eIs3c3k"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0,), (1,))])\n",
    "dataset = datasets.MNIST('~/data/', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58MA82hi5Wqo"
   },
   "source": [
    "Now, `federated_dataset` is a list of subsets of the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hmqxw8OtAJpe",
    "outputId": "fd4a61f6-7daa-4247-f3e4-04987e816213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of centers: 4\n"
     ]
    }
   ],
   "source": [
    "federated_dataset = split_iid(dataset, n_centers=N_CENTERS)\n",
    "print('Number of centers:', len(federated_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if3uAKOQ8gDA"
   },
   "source": [
    "## Defining and distributing a model: Variational Autoencoder\n",
    "In this excercise we will use the Multi-channel Variational Autoencoder proposed by _Antelmi et al (ICML 2019)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swxoLLB1HZjZ",
    "outputId": "8dc3a46e-6d7b-486e-e96b-3d54d3ebcad8"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://gitlab.inria.fr/epione_ML/mcvae.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fGYQqSqy8_Oe"
   },
   "outputs": [],
   "source": [
    "from mcvae.models import Mcvae, ThreeLayersVAE, VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0DULErY8ly-"
   },
   "source": [
    "First, it is necessary to define a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ugDT7HADNHyx"
   },
   "outputs": [],
   "source": [
    "N_FEATURES = 784  # Number of pixels in MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e28GtSTQ9r_t"
   },
   "outputs": [],
   "source": [
    "dummy_data = [torch.zeros(1, N_FEATURES)]  # Dummy data to initialize the input layer size\n",
    "lat_dim = 3  # Size of the latent space for this autoencoder\n",
    "vae_class = ThreeLayersVAE  # Architecture of the autoencoder (VAE: Single layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9PTIpaglAnM_"
   },
   "outputs": [],
   "source": [
    "model = Mcvae(data=dummy_data, lat_dim=lat_dim, vaeclass=vae_class)\n",
    "model.optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "model.init_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SY57ZUkx-YUc"
   },
   "source": [
    "Now replicate a copy of the models across different centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GiXQtB-eF3Mv"
   },
   "outputs": [],
   "source": [
    "models = [copy.deepcopy(model) for _ in range(N_CENTERS)]\n",
    "n_obs_per_client = [len(client_data) for client_data in federated_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tah_VnaB-xdo"
   },
   "source": [
    "Train in a federated fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ydgBCA49KQbO"
   },
   "outputs": [],
   "source": [
    "def get_data(subset, shuffle=True):\n",
    "    \"\"\" Extracts data from a Subset torch dataset in the form of a tensor\"\"\"\n",
    "    loader = DataLoader(subset, batch_size=len(subset), shuffle=shuffle)\n",
    "    return iter(loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uL2a10Rg-vE4",
    "outputId": "62704cff-e036-43d5-eb0c-94c655c3ddd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch:    0/15 (0%)\tLoss: 549.0060\tLL: -549.0015\tKL: 0.0044\tLL/KL: -123752.7716\n",
      "====> Epoch:   10/15 (67%)\tLoss: 82.7334\tLL: -78.5644\tKL: 4.1690\tLL/KL: -18.8448\n",
      "====> Epoch:    0/15 (0%)\tLoss: 551.0641\tLL: -551.0596\tKL: 0.0045\tLL/KL: -123185.0004\n",
      "====> Epoch:   10/15 (67%)\tLoss: 83.1423\tLL: -78.9854\tKL: 4.1570\tLL/KL: -19.0006\n",
      "====> Epoch:    0/15 (0%)\tLoss: 553.0543\tLL: -553.0498\tKL: 0.0045\tLL/KL: -123165.6171\n",
      "====> Epoch:   10/15 (67%)\tLoss: 83.6635\tLL: -79.2979\tKL: 4.3656\tLL/KL: -18.1644\n",
      "====> Epoch:    0/15 (0%)\tLoss: 551.7684\tLL: -551.7640\tKL: 0.0045\tLL/KL: -123816.6904\n",
      "====> Epoch:   10/15 (67%)\tLoss: 82.1891\tLL: -77.8627\tKL: 4.3264\tLL/KL: -17.9972\n",
      "====> Epoch:   20/30 (67%)\tLoss: 52.9532\tLL: -41.7303\tKL: 11.2229\tLL/KL: -3.7183\n",
      "====> Epoch:   20/30 (67%)\tLoss: 53.4088\tLL: -42.1911\tKL: 11.2177\tLL/KL: -3.7611\n",
      "====> Epoch:   20/30 (67%)\tLoss: 54.1244\tLL: -43.0145\tKL: 11.1099\tLL/KL: -3.8717\n",
      "====> Epoch:   20/30 (67%)\tLoss: 52.5903\tLL: -41.4866\tKL: 11.1037\tLL/KL: -3.7363\n",
      "====> Epoch:   30/45 (67%)\tLoss: 33.8065\tLL: -28.3495\tKL: 5.4571\tLL/KL: -5.1950\n",
      "====> Epoch:   40/45 (89%)\tLoss: 18.1020\tLL: -13.2426\tKL: 4.8594\tLL/KL: -2.7252\n",
      "====> Epoch:   30/45 (67%)\tLoss: 34.1739\tLL: -28.6980\tKL: 5.4759\tLL/KL: -5.2408\n",
      "====> Epoch:   40/45 (89%)\tLoss: 18.5314\tLL: -13.6631\tKL: 4.8683\tLL/KL: -2.8066\n",
      "====> Epoch:   30/45 (67%)\tLoss: 34.6137\tLL: -29.1391\tKL: 5.4745\tLL/KL: -5.3227\n",
      "====> Epoch:   40/45 (89%)\tLoss: 19.2687\tLL: -14.3770\tKL: 4.8917\tLL/KL: -2.9391\n",
      "====> Epoch:   30/45 (67%)\tLoss: 33.2370\tLL: -27.7640\tKL: 5.4730\tLL/KL: -5.0729\n",
      "====> Epoch:   40/45 (89%)\tLoss: 17.4709\tLL: -12.5999\tKL: 4.8710\tLL/KL: -2.5867\n",
      "====> Epoch:   50/60 (83%)\tLoss: -5.0630\tLL: 11.4165\tKL: 6.3535\tLL/KL: 1.7969\n",
      "====> Epoch:   50/60 (83%)\tLoss: -4.9994\tLL: 11.3325\tKL: 6.3331\tLL/KL: 1.7894\n",
      "====> Epoch:   50/60 (83%)\tLoss: -4.3881\tLL: 10.7360\tKL: 6.3479\tLL/KL: 1.6913\n",
      "====> Epoch:   50/60 (83%)\tLoss: -6.0503\tLL: 12.3884\tKL: 6.3382\tLL/KL: 1.9546\n",
      "====> Epoch:   60/75 (80%)\tLoss: -29.5380\tLL: 36.7966\tKL: 7.2586\tLL/KL: 5.0694\n",
      "====> Epoch:   70/75 (93%)\tLoss: -55.2589\tLL: 63.4622\tKL: 8.2032\tLL/KL: 7.7362\n",
      "====> Epoch:   60/75 (80%)\tLoss: -29.4960\tLL: 36.7787\tKL: 7.2827\tLL/KL: 5.0501\n",
      "====> Epoch:   70/75 (93%)\tLoss: -55.1117\tLL: 63.3170\tKL: 8.2053\tLL/KL: 7.7166\n",
      "====> Epoch:   60/75 (80%)\tLoss: -28.9316\tLL: 36.2149\tKL: 7.2833\tLL/KL: 4.9723\n",
      "====> Epoch:   70/75 (93%)\tLoss: -54.1728\tLL: 62.3446\tKL: 8.1718\tLL/KL: 7.6292\n",
      "====> Epoch:   60/75 (80%)\tLoss: -30.7607\tLL: 38.0435\tKL: 7.2829\tLL/KL: 5.2237\n",
      "====> Epoch:   70/75 (93%)\tLoss: -56.6350\tLL: 64.8457\tKL: 8.2107\tLL/KL: 7.8977\n",
      "====> Epoch:   80/90 (89%)\tLoss: -72.8661\tLL: 81.1838\tKL: 8.3177\tLL/KL: 9.7603\n",
      "====> Epoch:   80/90 (89%)\tLoss: -73.5662\tLL: 81.9328\tKL: 8.3665\tLL/KL: 9.7929\n",
      "====> Epoch:   80/90 (89%)\tLoss: -72.2818\tLL: 80.6926\tKL: 8.4108\tLL/KL: 9.5939\n",
      "====> Epoch:   80/90 (89%)\tLoss: -74.6665\tLL: 83.0156\tKL: 8.3490\tLL/KL: 9.9431\n",
      "====> Epoch:   90/105 (86%)\tLoss: -85.3397\tLL: 94.0086\tKL: 8.6689\tLL/KL: 10.8444\n",
      "====> Epoch:  100/105 (95%)\tLoss: -98.8286\tLL: 107.4869\tKL: 8.6583\tLL/KL: 12.4143\n",
      "====> Epoch:   90/105 (86%)\tLoss: -85.8566\tLL: 94.5646\tKL: 8.7080\tLL/KL: 10.8595\n",
      "====> Epoch:  100/105 (95%)\tLoss: -98.9881\tLL: 107.6321\tKL: 8.6441\tLL/KL: 12.4516\n",
      "====> Epoch:   90/105 (86%)\tLoss: -84.7209\tLL: 93.4080\tKL: 8.6871\tLL/KL: 10.7525\n",
      "====> Epoch:  100/105 (95%)\tLoss: -97.8231\tLL: 106.4837\tKL: 8.6606\tLL/KL: 12.2952\n",
      "====> Epoch:   90/105 (86%)\tLoss: -86.8344\tLL: 95.5405\tKL: 8.7061\tLL/KL: 10.9740\n",
      "====> Epoch:  100/105 (95%)\tLoss: -100.0892\tLL: 108.7154\tKL: 8.6262\tLL/KL: 12.6030\n",
      "====> Epoch:  110/120 (92%)\tLoss: -112.8582\tLL: 121.6993\tKL: 8.8411\tLL/KL: 13.7653\n",
      "====> Epoch:  110/120 (92%)\tLoss: -112.9543\tLL: 121.8334\tKL: 8.8791\tLL/KL: 13.7214\n",
      "====> Epoch:  110/120 (92%)\tLoss: -112.0804\tLL: 120.9473\tKL: 8.8668\tLL/KL: 13.6404\n",
      "====> Epoch:  110/120 (92%)\tLoss: -114.0907\tLL: 122.8925\tKL: 8.8018\tLL/KL: 13.9622\n",
      "====> Epoch:  120/135 (89%)\tLoss: -126.3037\tLL: 135.2220\tKL: 8.9182\tLL/KL: 15.1624\n",
      "====> Epoch:  130/135 (96%)\tLoss: -138.9217\tLL: 147.9530\tKL: 9.0313\tLL/KL: 16.3823\n",
      "====> Epoch:  120/135 (89%)\tLoss: -126.2420\tLL: 135.1894\tKL: 8.9473\tLL/KL: 15.1095\n",
      "====> Epoch:  130/135 (96%)\tLoss: -138.2348\tLL: 147.2517\tKL: 9.0168\tLL/KL: 16.3308\n",
      "====> Epoch:  120/135 (89%)\tLoss: -125.5312\tLL: 134.4658\tKL: 8.9346\tLL/KL: 15.0500\n",
      "====> Epoch:  130/135 (96%)\tLoss: -137.8430\tLL: 146.8761\tKL: 9.0331\tLL/KL: 16.2597\n",
      "====> Epoch:  120/135 (89%)\tLoss: -127.3756\tLL: 136.3297\tKL: 8.9540\tLL/KL: 15.2255\n",
      "====> Epoch:  130/135 (96%)\tLoss: -139.6181\tLL: 148.6477\tKL: 9.0296\tLL/KL: 16.4622\n",
      "====> Epoch:  140/150 (93%)\tLoss: -149.4960\tLL: 158.6330\tKL: 9.1369\tLL/KL: 17.3618\n",
      "====> Epoch:  140/150 (93%)\tLoss: -149.3842\tLL: 158.5509\tKL: 9.1666\tLL/KL: 17.2966\n",
      "====> Epoch:  140/150 (93%)\tLoss: -148.9252\tLL: 158.0730\tKL: 9.1477\tLL/KL: 17.2801\n",
      "====> Epoch:  140/150 (93%)\tLoss: -150.6223\tLL: 159.7808\tKL: 9.1586\tLL/KL: 17.4460\n"
     ]
    }
   ],
   "source": [
    "init_params = model.state_dict()\n",
    "for round_i in range(N_ROUNDS):\n",
    "    for client_dataset, client_model in zip(federated_dataset, models):\n",
    "        # Load client data in the form of a tensor\n",
    "        X, y = get_data(client_dataset)\n",
    "        client_model.data = [X.view(-1, N_FEATURES)]  # Set data attribute in client's model (list wraps the number of channels)\n",
    "\n",
    "        # Load client's model parameters and train\n",
    "        client_model.load_state_dict(init_params)\n",
    "        client_model.optimize(epochs=N_EPOCHS, data=client_model.data)\n",
    "        \n",
    "    # Aggregate models using federated averaging\n",
    "    trained_model = federated_averaging(models, n_obs_per_client)\n",
    "    init_params = trained_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gz_IRXHOL_L"
   },
   "source": [
    "## Results visualization\n",
    "Using the final parameters we can evaluate the performance of the model by visualizing the testing set onto the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-yNMU8hoPGjV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5CABgW5RFY7c"
   },
   "outputs": [],
   "source": [
    "dataset_test = datasets.MNIST('~/data/', train=False, download=True)\n",
    "X_test, y_test = [dataset_test.data.view(-1, N_FEATURES).float()], dataset_test.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7zLZGrUkO50c"
   },
   "outputs": [],
   "source": [
    "Z_test = np.hstack([z.loc.detach().numpy() for z in trained_model.encode(X_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE A PANDA DATAFRAME WITH FEATURES: LATENT COORDINATES + LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###PLOT THE LATENT SPACE WITH A PAIRPLOT USING THE CREATED DATAFRAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "federated-mcvae.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
